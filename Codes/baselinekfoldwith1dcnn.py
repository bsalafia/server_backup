# -*- coding: utf-8 -*-
"""BaseLineKfoldWith1DCNNColab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KxPYDmE1Fnk9JJv7dRUXCLVMu_wCu-Vi
"""

# Commented out IPython magic to ensure Python compatibility.
import os
# os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
# os.environ["CUDA_VISIBLE_DEVICES"]="3"
from keras.models import Model
from keras.utils.generic_utils import get_custom_objects
from keras import optimizers, regularizers
import keras.backend as K
from keras import regularizers
from tensorflow.keras.layers import InputLayer
from keras.layers import Input

import tensorflow as tf

import scipy
import h5py
import glob
# import BaseLineModel
from scipy.io import loadmat
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
# from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten,Concatenate

from keras.optimizers import Adam
from keras.layers.normalization import BatchNormalization
from keras.utils import np_utils
from keras.layers import Conv1D, MaxPooling1D, ZeroPadding1D, GlobalAveragePooling1D
from keras.layers.advanced_activations import LeakyReLU
from keras.preprocessing.image import ImageDataGenerator
from sklearn import preprocessing
from keras import regularizers
from numpy import mean
from numpy import std
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.layers import InputLayer
from keras.layers import Input
# from google.colab import drive
from sklearn.model_selection import LeaveOneOut
import gc
gc.collect()
# drive.mount('/content/drive')
#
# /content/drive')

def LoadData(dirname,indx):

    os.chdir(dirname)
    a=[]
    X=[]
    Y=[]
    k=0
    for file in glob.glob("*.mat"):

        a.append(file)
    Name=[a[x] for x in indx]
    for i in range(len(Name)):

        matFile1 = h5py.File(os.path.join(dirname, Name[i]),'r')
        xx=matFile1.get('input')
        yy=matFile1.get('target')
        x1=np.array(xx)
        y1=np.array(yy)
        x= np.transpose(x1)
        y=np.transpose(y1)
        step=x.shape[0]
        X.append(x)
        Y.append(y)
        k=k+step

    xx = np.concatenate(X, axis=0)
    yy = np.concatenate(Y, axis=0)
    return xx[:,1:,:],yy

# version 1: filters = 5 kernelsize = 9
## Version2
#     filter1=8
#     filter2=16

#     kernelsize1=7
#     kernelsize2=9
## Version3
#     filter1=8
#     filter2=16

#     kernelsize1=10
#     kernelsize2=22

## Version4
#     filter1=8 ---> at the first CNN and only two dense
#     filter2=16

#     kernelsize1=10
#     kernelsize2=22

## Version5
#     filter1=8 ---> at the first CNN and only two dense
#     filter2=16

#     kernelsize1=22
#     kernelsize2=10

def define_model():


    model = Sequential()

    filter1=8
    filter2=16

    kernelsize1=22
    kernelsize2=10

    model.add(Conv1D(filter1, kernelsize2, input_shape=(1024,18)))
    model.add(Conv1D(filter1, kernelsize1))
    model.add(BatchNormalization(axis=-1))
    model.add(Activation('relu'))
    model.add(MaxPooling1D(pool_size=2))

    model.add(Conv1D(filter1, kernelsize2))
    model.add(Conv1D(filter1, kernelsize1))
    model.add(BatchNormalization(axis=-1))
    model.add(Activation('relu'))
    model.add(MaxPooling1D(pool_size=2))

    model.add(Conv1D(filter1, kernelsize2))
    model.add(Conv1D(filter1, kernelsize1))
    model.add(BatchNormalization(axis=-1))
    model.add(Activation('relu'))
    model.add(MaxPooling1D(pool_size=2))

    model.add(Conv1D(filter2, kernelsize2))
    model.add(Conv1D(filter2, kernelsize1))
    model.add(BatchNormalization(axis=-1))
    model.add(Activation('relu'))
    model.add(MaxPooling1D(pool_size=2))

    model.add(Conv1D(filter2, kernelsize2))
    model.add(Conv1D(filter2, kernelsize1))
    model.add(BatchNormalization(axis=-1))
    model.add(Activation('relu'))
    model.add(MaxPooling1D(pool_size=2))

    model.add(Conv1D(filter1, 1))

    model.add(Dropout(0.25))
    model.add(Flatten())

# Fully connected layer

    model.add(Dense(8))
    model.add(Dense(8))
    model.add(Dropout(0.5))

    model.add(Dense(1))
    model.add(Activation('sigmoid'))
    model.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy', metrics=['accuracy'])

    return model

dirname='/home/baharsalafian/1DCNNDataset'
SaveResults='/home/baharsalafian/PreviousCNNs'

import time
start_time = time.time()
fold_no = 1
epoch = 10
batchsize = 128
FoldNum=6

model=define_model()
# Define the K-fold Cross Validator
kfold = KFold(n_splits=FoldNum, shuffle=False)

score=[]
loss1=[]
for trainindx, testindx in kfold.split(range(24)):
    X_train,Y_train=LoadData(dirname,trainindx)
    X_test,Y_test=LoadData(dirname,testindx)
    model.fit(X_train,Y_train,validation_split=0.2,batch_size=batchsize , epochs=epoch,verbose = 2)

    X_train = None
    Y_train = None
    gc.collect()

    loss, acc = model.evaluate(X_test,Y_test, verbose=2)
    score.append(acc)
    loss1.append(loss)
    model.save('/home/baharsalafian/PreviousCNNs/6FoldModelCNNV3'+ str(fold_no)+'.h5')
    fold_no=fold_no+1
    np.save(os.path.join(SaveResults,'X_testV3_'+str(fold_no)),X_test)
    np.save(os.path.join(SaveResults,'Y_testV3_'+str(fold_no)),Y_test)

    X_test = None
    Y_test = None
    gc.collect()

print("--- %s seconds ---" % (time.time() - start_time))

np.save(os.path.join(SaveResults,'accuracyCNNV2'),score)
np.save(os.path.join(SaveResults,'lossCNNV2'),loss1)
