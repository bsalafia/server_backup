# -*- coding: utf-8 -*-
"""Pybilt_Bahareh.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ket1xz-dzItrE4NWlA5EHo0lAl0e1p1L
"""

# Commented out IPython magic to ensure Python compatibility.
# import tensorflow as tf
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import os
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID" 
os.environ["CUDA_VISIBLE_DEVICES"]="4"
import scipy
import h5py
import glob, os
from scipy.io import loadmat,savemat
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
from scipy.special import gamma,psi
from scipy.spatial.distance import cdist
from six.moves import range
from sklearn import preprocessing
from keras import regularizers
from numpy import mean
from numpy import std
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from tqdm.auto import tqdm, trange
from numpy.random import default_rng
# import torch.nn.functional as F
# from google.colab import drive
import gc
import time
# gc.collect()
# drive.mount('/content/drive')
# print(np.version.version)

def LoadData(dirname,indx):

    os.chdir(dirname)
    a=[]
    X=[]
    Y=[]
    k=0
    for file in glob.glob("*.mat"):
        
        a.append(file)
    # print(a)
    Name=[a[x] for x in indx]
    for i in range(len(Name)):
        
        matFile1 = h5py.File(os.path.join(dirname, Name[i]),'r')
        xx=matFile1.get('input')
        yy=matFile1.get('target')
        x1=np.array(xx)
        y1=np.array(yy)
        x= np.transpose(x1)
        y=np.transpose(y1)
        step=x.shape[0]
        X.append(x)
        Y.append(y)
        k=k+step
        
    xx = np.concatenate(X, axis=0)
    yy = np.concatenate(Y, axis=0)
    return xx,yy

def k_nearest_neighbors(X, k=1):
    """Get the k-nearest neighbors between points in a random variable/vector.

    Determines the k nearest neighbors for each point in teh random
    variable/vector using Euclidean style distances.

    Args:
        X (np.array): A random variable of form X = {x_1, x_2, x_3,...,x_N} or
            a random vector of form X = {(x_1, y_1), (x_2, y_2),...,(x_N,
            y_n)}.
        k (Optional[int]): The number of nearest neighbors to store for each
            point. Defaults to 1.

    Returns:
        dict: A dictionary keyed by the indices of X and containing a list
            of the k nearest neighbor for each point along with the distance
            value between the point and the nearest neighbor.
    """
    nX = len(X)
    # initialize knn dict
    knn = {key: [] for key in range(nX)}
    # make sure X has the right shape for the cdist function
    X = np.reshape(X, (nX,-1))
    dists_arr = cdist(X, X)
    distances = [[i,j,dists_arr[i,j]] for i in range(nX-1) for j in range(i+1,nX)]
    # sort distances
    distances.sort(key=lambda x: x[2])
    # pick up the k nearest
    for d in distances:
        i = d[0]
        j = d[1]
        dist = d[2]
        if len(knn[i]) < k:
            knn[i].append([j, dist])
        if len(knn[j]) < k:
            knn[j].append([i, dist])
    return knn

def kth_nearest_neighbor_distances(X, k=1):
    """Returns the distance for the kth nearest neighbor of each point.

   Args:
        X (np.array): A random variable of form X = {x_1, x_2, x_3,...,x_N} or
            a random vector of form X = {(x_1, y_1), (x_2, y_2),...,(x_N,
            y_n)}.
        k (Optional[int]): The number of nearest neighbors to check for each
            point. Defaults to 1.
    Returns:
        list: A list in same order as X with the distance value to the kth
        nearest neighbor of each point in X.

    """
    nX = len(X)
    # make sure X has the right shape for the cdist function
    X = np.reshape(X, (nX,-1))
    dists_arr = cdist(X, X)
    # sorts each row
    dists_arr.sort()
    return [dists_arr[i][k] for i in range(nX)]

def shannon_entropy(X, k=1, kth_dists=None):
    """Return the Shannon Entropy of the random variable/vector.

    This function computes the Shannon information entropy of the
    random variable/vector as estimated using the Kozachenko-Leonenko (KL)
    knn estimator.

    Args:
        X (np.array): A random variable of form X = {x_1, x_2, x_3,...,x_N} or
            a random vector of form X = {(x_1, y_1), (x_2, y_2),...,(x_N,
            y_n)}.
        k (Optional[int]): The number of nearest neighbors to store for each
            point. Defaults to 1.
        kth_dists (Optional[list]): A list in the same order as points in X
            that has the pre-computed distances between the points in X and
            their kth nearest neighbors at. Defaults to None.

    References:
        1. Damiano Lombardi and Sanjay Pant, A non-parametric k-nearest
            neighbour entropy estimator, arXiv preprint,
                [cs.IT] 2015, arXiv:1506.06501v1.
                https://arxiv.org/pdf/1506.06501v1.pdf

        2. https://www.cs.tut.fi/~timhome/tim/tim/core/differential_entropy_kl_details.htm

        3. Kozachenko, L. F. & Leonenko, N. N. 1987 Sample estimate of entropy
            of a random vector. Probl. Inf. Transm. 23, 95-101.

    Returns:
        float: The estimate of the Shannon Information entropy of X.

    """
    # the kth nearest neighbor distances
    r_k = kth_dists
    if kth_dists is None:
        r_k = kth_nearest_neighbor_distances(X, k=k)
    # length
    n = len(X)
    # dimension
    d = 1
    if len(X.shape) == 2:
        d = X.shape[1]
    # volume of unit ball in d^n
    v_unit_ball = np.pi**(0.5*d)/gamma(0.5*d + 1.0)
    # log distances
    lr_k = np.log(r_k)
    # Shannon entropy estimate
    H = psi(n) - psi(k) + np.log(v_unit_ball) + (np.float(d)/np.float(n))*( lr_k.sum())

    return H

def shannon_entropy_pc(X, k=1, kth_dists=None):
    """Return the Shannon Entropy of the random variable/vector.

    This function computes the Shannon information entropy of the
    random variable/vector as estimated using the Perez-Cruz knn estimator
    described in Reference 1.

    Args:
        X (np.array): A random variable of form X = {x_1, x_2, x_3,...,x_N} or
            a random vector of form X = {(x_1, y_1), (x_2, y_2),...,(x_N,
            y_n)}.
        k (Optional[int]): The number of nearest neighbors to store for each
            point. Defaults to 1.
        kth_dists (Optional[list]): A list in the same order as points in X
            that has the pre-computed distances between the points in X and
            their kth nearest neighbors at. Defaults to None.

    References:
        1. Perez-Cruz, (2008). Estimation of Information Theoretic Measures
            for Continuous Random Variables. Advances in Neural Information
            Processing Systems 21 (NIPS). Vancouver (Canada), December.
            https://papers.nips.cc/paper/3417-estimation-of-information-theoretic-measures-for-continuous-random-variables.pdf

    Returns:
        float: The estimate of the Shannon Information entropy of X.

    """
    r_k = kth_dists
    if kth_dists is None:
        r_k = np.array(kth_nearest_neighbor_distances(X, k=k))
    n = len(X)
    d = 1
    if len(X.shape) == 2:
        d = X.shape[1]
    #volume of the unit ball
    v_unit_ball = np.pi**(0.5*d)/gamma(0.5*d + 1.0)
    # probability estimator using knn distances
    p_k_hat = (k / (n -1.0)) * (1.0/v_unit_ball) * (1.0/r_k**d)
    # log probability
    log_p_k_hat = np.log(p_k_hat)
    #entropy estimator
    h_k_hat = log_p_k_hat.sum() / (-1.0*n)
    return h_k_hat

def mutual_information(var_tuple, k=2):
    """Returns an estimate of the mutual information.

    This function computes an estimate of the mutual information between a
    set of random variables or random vectors using knn estimators for the
    entropy calculations.

    Args:
        var_tuple (tuple): A tuple of random variables or random
            vectors (i.e. numpy.array); e.g. var_tuple = (X, Y) where X form
            X = {x_1, x_2, x_3,...,x_N} and Y has form
            Y = {x_1, x_2, x_3,...,x_N}, or where X has form
            X = {(x_X1, y_X1), (x_X2, y_X2),...,(x_XN, y_XN)} and Y has form
            Y = {(x_Y1, y_Y1), (x_Y2, y_Y2),...,(x_YN, y_YN)}.
         k (Optional[int]): The number of nearest neighbors to store for each
             point. Defaults to 2.

    Returns:
        float: The mutual information estimate.

    Notes:
        The information entropies used to estimate the mutual information
        are computed using the shannon_entropy function. All input random
        variable/vector arrays must have the same shape.

    """
    nvar = len(var_tuple)
    # make sure the input arrays are properly shaped for hstacking
    var_tuple = tuple( var_tuple[i].reshape(len(var_tuple[i]),-1) for i in range(nvar) )
    # compute the individual entropies of each variable
    Hx = [shannon_entropy(var_tuple[i],k=k) for i in range(nvar)]
    Hx = np.array(Hx)
    # and get the sum
    Hxtot = Hx.sum()
    # now get the entropy of the joint distribution
    joint = np.hstack(var_tuple)
    Hjoint = shannon_entropy(joint, k=k)
    # get the mutual information
    MI = Hxtot - Hjoint
    # set to zero if value is negative
    if MI < 0.0: MI = 0.0
    # return
    return MI

def normalize(data):
  signal_max = np.max(data,axis=(0,1))
  ##########
  signal_min=np.min(data,axis=(0,1))
  den=signal_max-signal_min
  if data.ndim > 1:
    den[den == 0] = 1.
  elif den == 0:
    den = 1.
    
  return signal_max,signal_min,den

def standardize(data):
  mean1 = np.mean(data,axis=(0,1))
  stdv = np.std(data,axis=(0,1))
  
  if data.ndim > 1:
      stdv[stdv == 0] = 1.
  elif stdv == 0:
      stdv = 1.
  return mean1,stdv

dirname='/home/baharsalafian/1DCNNDataset'
dir_mi_results = '/home/baharsalafian/MI_KNN_results'

start_time = time.time()
labels = [0, 1]
for file_idex in range(24):
  X_train,Y_train=LoadData(dirname, [file_idex])

##################
  signal_max,signal_min,den=normalize(X_train)

  mean1,stdv=standardize(X_train)
###############

  #X_train=low_amplitude_noise(X_train, eps=1e-10)

  estimated_MI = np.zeros((len(labels), X_train.shape[2], X_train.shape[2]))
  estimated_MI_var= np.zeros((len(labels), X_train.shape[2], X_train.shape[2]))
  for l in labels:
    for j in range(X_train.shape[2]-1):
      for k in range(j+1, X_train.shape[2]):
        
        all_x = []
        all_y = []
        for i in tqdm(range(X_train.shape[0])):
          if Y_train[i] == l:
            continue
          estimated_mi=np.zeros((X_train.shape[0]))
          X =X_train[i, :, j]/signal_max[j]
          Y = X_train[i, :, k]/signal_max[k]
          X_in=(X,Y)
          estimated_mi[i]=mutual_information(X_in)

        estimated_MI[l,j,k]=np.mean(estimated_mi)
        estimated_MI_var[l,j,k]=np.var(estimated_mi) 
 
        file_name = 'Pybilt_MeanMI_Est_1NNperChan_fileidx_{}.mat'.format(file_idex)
        savemat(os.path.join(dir_mi_results, file_name), {"estimated_MI": estimated_MI})

        file_name2 = 'Pybilt_VarMI_Est_1NNperChan_fileidx_{}.mat'.format(file_idex)
        savemat(os.path.join(dir_mi_results, file_name2), {"estimated_MI": estimated_MI_var})
        print("-----File_idx {}, Label {}, Chan1 {},  Chan2 {}, Estimated MI {:.4f}".format
              (file_idex, l, j, k, estimated_MI[l,j,k]))  

    
    plt.imshow(estimated_MI[l])
    plt.show()      

print("--- %s seconds ---" % (time.time() - start_time))

a=np.nonzero(Y_train)
print(a)

b=np.array([[1,2,3],[2,3,7],[3,7,8]])
b.shape
print(np.mean(b[-50:]))