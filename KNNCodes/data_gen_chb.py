# -*- coding: utf-8 -*-
"""data_gen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ItQk7gSp7GxCtYja70FssZEht1XLDqU_
"""

import pandas as pd
import cv2
import numpy as np
from sklearn.utils import shuffle
import os
from collections import deque
from scipy.io import loadmat
import copy
import matplotlib
import matplotlib.pyplot as plt
from keras.utils import np_utils
import pickle
class SeizureDataGenerator(object):

  def __init__(self,dir_csv,temporal_stride=1,temporal_length=8,shuffle=True):

    self.dir_csv = dir_csv
    self.temporal_stride = temporal_stride
    self.temporal_length = temporal_length
    self.shuffle = shuffle

  def shuffle_data(self,samples):

    data = shuffle(samples,random_state=2)
    return data
    

  def segment_data(self,loaddir):

    info_mat = loadmat(loaddir)
    data = info_mat['data']
    DenoisedSig = info_mat['DenoisedSig']
    DenoisedSigSeizure = info_mat['DenoisedSigSeizure']

    #print("shape of data is ", data.shape)
    #print("shape of DenoisedSig is ", DenoisedSig.shape)


    SigEnd = info_mat['Sig_end'][0][0]
    SigStart = info_mat['Sig_start'][0][0]
    SiezureStart = info_mat['Siezure_start'][0][0]
    SiezureEnd = info_mat['Siezure_end'][0][0]
    n_channels = 18
    Fs = 256

    data_denoised = DenoisedSig[:,0:DenoisedSig.shape[1]-1]

    n = data_denoised.shape[1]//Fs

    output=[data_denoised[:,i:i + Fs] for i in range(0,data_denoised.shape[1], Fs)]

    X = np.array(output)
    #print("out shape", X.shape)

    # Ylabel=np.zeros((SigEnd-SigStart,1))
    # Seizure_durarion = SiezureEnd - SiezureStart + 1

    # Seizure_start_label = SiezureStart - SigStart 
    # #print(Seizure_start_label)
    # Ylabel[Seizure_start_label:Seizure_start_label+Seizure_durarion,0] =  1

    return X



  def file_generator(self,data_path,data_files):

    '''
    data_files - list of csv files to be read.
    '''
    for f in data_files:
      # read all the csv files (one csv file corresponds to one vdieo) in data_files one by one
      tmp_df = pd.read_csv(os.path.join(data_path,f))
      label_list = list(tmp_df['Label'])  # Load all the labels in the label_list
      index_list = list(tmp_df['index'])
      total_images = len(label_list)
      if total_images>=self.temporal_length: # only if the number of frames in the video is greater tha temporal length, use that video
        num_samples = int((total_images-self.temporal_length)/self.temporal_stride)+1
        print ('num of samples from vid seq-{}: {}'.format(f,num_samples))
        img_list = list(tmp_df['FileName'])
      else: # if the number of frames are less than temporal length , discard it
        print ('num of frames is less than temporal length; hence discarding this file-{}'.format(f))
        continue

      start_frame = 0
      samples = deque() # initliaze a queue to store the frames
      labels = deque()
      indices = deque()
      samp_count=0 # a counter to count the number of smaple. one smaple has as many frames as defined by temporal length
      for j  in range(len(img_list)):

        samples.append(img_list[j])
        labels.append(label_list[j])
        indices.append(index_list[j])
        if len(samples)==self.temporal_length:
          #if the queue has as many frames as temporal length, return it as one sample

          samples_c = copy.deepcopy(samples)
          lables_c = copy.deepcopy(labels) # copy the queue as in the next stage frames would be popped
          indices_c = copy.deepcopy(indices)
          samp_count+=1

          for t in range(self.temporal_stride): # pop out as many frames as described by the stride from the left to accomodate new frames
            samples.popleft()
            labels.popleft()
            indices_c.pop
            # print(list(indices_c).pop(0))

          yield samples_c, list(lables_c), list(indices_c)[-self.temporal_length:]# return a sample(consisting of as many frames as defined by temporal length)
                                            # and its corsponding label

  def load_samples_modified(self,data_cat,num_csv):

    data_path = os.path.join(self.dir_csv,f'data_files_chb/{data_cat}')
    data_csv_list = os.listdir(data_path)

    if self.shuffle:
      data_shuffle = self.shuffle_data(data_csv_list)

    data_files = data_shuffle#[0:num_csv]
    # define a generator to read the samples
    file_gen = self.file_generator(data_path,data_files)
    iterator = True
    data_list = []
    while iterator:
        try:
            x, y, index = next(file_gen)
            x=list(x)
            data_list.append([x,y,index])
        except Exception as e:
            print ('the exception: ',e)
            iterator = False
            print ('end of data generator')
    return data_list


  def load_samples_modified_V2(self,data_cat,num_csv):

    data_path = os.path.join(self.dir_csv,f'data_files_chb/{data_cat}')
    data_csv_list = os.listdir(data_path)
    if self.shuffle:

      data_shuffle = self.shuffle_data(data_csv_list)

    data_files = data_shuffle[0:num_csv]
    iterator = True
    for i in range(len(data_files)):

      while iterator:


        # define a generator to read the samples
        file_gen = self.file_generator(data_path,data_files)
        data_list = []
        try:
            x, y, index = next(file_gen)
            x=list(x)
            data_list.append([x,y,index])
        except Exception as e:
            print ('the exception: ',e)
            iterator = False
            print ('end of data generator')
      return data_list


  def data_generator_modified(self,data_cat,num_csv,batch_size=256,shuffle=True):

    """
    Yields the next training batch.
    Suppose `samples` is an array [[image1_filename,label1], [image2_filename,label2],...].
    """
    data = self.load_samples_modified(data_cat,num_csv)
    num_samples = len(data)
    if self.shuffle:
        data = self.shuffle_data(data)
    while True:

      for offset in range(0, (num_samples//batch_size)*batch_size, batch_size):
        # print ('startring index: ', offset)
        # Get the samples you'll use in this batch
        batch_samples = data[offset:offset+batch_size]
        # Initialise X_train and y_train arrays for this batch
        x_train = []
        y_train = []
        # For each example
        for batch_sample in batch_samples:

          # Load image (X)

          y = batch_sample[1]
          x = self.segment_data(batch_sample[0][0])
          ind = batch_sample[2]
          # print(ind)
          x_train.append(x[ind])
          y_train.append(y)

        # Make sure they're numpy arrays (as opposed to lists)
        x_train = np.array(x_train)
        #X_train = np.rollaxis(X_train,1,4)
        y_train = np.array(y_train)
        # The generator-y part: yield the next training batch
        yield x_train, y_train
